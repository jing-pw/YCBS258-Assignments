{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 CNN Improved.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jing-pw/YCBS258-Assignments/blob/master/CIFAR10_CNN_Improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7fT1hhas_ng",
        "colab_type": "code",
        "outputId": "d3045e8d-0ef7-4648-ed64-fe2613bd3bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003        \n",
        "    return lrate\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "input_shape = x_train.shape[1:]\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay), \n",
        "                 input_shape=input_shape))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', \n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#training\n",
        "batch_size = 64\n",
        "\n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\n",
        "                    verbose=1,validation_data=(x_test,y_test),\n",
        "                    callbacks=[LearningRateScheduler(lr_schedule),\n",
        "                              EarlyStopping(monitor='val_acc', mode='max', \n",
        "                                            patience=5)])\n",
        "\n",
        "#save to disk\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights('model.h5')    \n",
        "\n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0628 06:27:47.381198 140058457139072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/125\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 1.9131 - acc: 0.4291 - val_loss: 1.5743 - val_acc: 0.5020\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.3869 - acc: 0.5825 - val_loss: 2.2054 - val_acc: 0.5574\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.2229 - acc: 0.6359 - val_loss: 1.2540 - val_acc: 0.6206\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.0741 - acc: 0.6758 - val_loss: 0.9505 - val_acc: 0.7227\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.9808 - acc: 0.7026 - val_loss: 0.9905 - val_acc: 0.7085\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.9237 - acc: 0.7196 - val_loss: 0.8243 - val_acc: 0.7581\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8714 - acc: 0.7385 - val_loss: 0.8946 - val_acc: 0.7392\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8437 - acc: 0.7491 - val_loss: 0.8462 - val_acc: 0.7610\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8248 - acc: 0.7562 - val_loss: 0.7428 - val_acc: 0.7862\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7954 - acc: 0.7663 - val_loss: 0.9114 - val_acc: 0.7421\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7799 - acc: 0.7712 - val_loss: 0.8382 - val_acc: 0.7602\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7584 - acc: 0.7808 - val_loss: 0.7470 - val_acc: 0.7968\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7437 - acc: 0.7862 - val_loss: 0.7028 - val_acc: 0.8078\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7383 - acc: 0.7899 - val_loss: 0.6850 - val_acc: 0.8157\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7273 - acc: 0.7946 - val_loss: 0.6849 - val_acc: 0.8135\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7191 - acc: 0.7983 - val_loss: 0.6839 - val_acc: 0.8174\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7043 - acc: 0.8037 - val_loss: 0.8558 - val_acc: 0.7737\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6981 - acc: 0.8053 - val_loss: 0.6922 - val_acc: 0.8093\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6900 - acc: 0.8103 - val_loss: 0.6997 - val_acc: 0.8117\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6882 - acc: 0.8116 - val_loss: 0.6719 - val_acc: 0.8164\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6791 - acc: 0.8136 - val_loss: 0.6690 - val_acc: 0.8231\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6717 - acc: 0.8176 - val_loss: 0.6483 - val_acc: 0.8279\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6712 - acc: 0.8173 - val_loss: 0.6563 - val_acc: 0.8283\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6631 - acc: 0.8178 - val_loss: 0.6559 - val_acc: 0.8335\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6616 - acc: 0.8193 - val_loss: 0.7690 - val_acc: 0.7984\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6571 - acc: 0.8222 - val_loss: 0.7067 - val_acc: 0.8102\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6534 - acc: 0.8262 - val_loss: 0.6373 - val_acc: 0.8377\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6468 - acc: 0.8268 - val_loss: 0.6867 - val_acc: 0.8185\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6457 - acc: 0.8260 - val_loss: 0.6526 - val_acc: 0.8282\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6448 - acc: 0.8278 - val_loss: 0.6942 - val_acc: 0.8227\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6426 - acc: 0.8299 - val_loss: 0.6663 - val_acc: 0.8319\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.6420 - acc: 0.8275 - val_loss: 0.6515 - val_acc: 0.8372\n",
            "10000/10000 [==============================] - 1s 66us/step\n",
            "\n",
            "Test result: 83.720 loss: 0.651\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}